{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b4a3b1c-73a8-4667-a4ea-557af35ce2b2",
   "metadata": {},
   "source": [
    "## **Linear Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc2ec3d5-f0e2-4457-b0a8-02d2a3acb220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class MyLineReg:\n",
    "    def __init__(self, n_iter, learning_rate, weights=None, metric=None, reg=None, l1_coef=0, l2_coef=0, sgd_sample=None, random_state=42):\n",
    "        self.n_iter = n_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = weights\n",
    "        self.metric = metric\n",
    "        self.best_metric = None\n",
    "        self.l1_coef = l1_coef\n",
    "        self.l2_coef = l2_coef\n",
    "        self.reg = reg\n",
    "        self.sgd_sample = sgd_sample\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"MyLineReg class: \\n{' '.join([f'{key} = {value}' for key, value in self.__dict__.items()])}\"\n",
    "\n",
    "    def __metric(self, y_train, y_pred):\n",
    "        if self.metric == 'mae':\n",
    "            return (np.abs(y_train - y_pred)).mean()\n",
    "        if self.metric == 'mse':\n",
    "            return ((y_train - y_pred) ** 2).mean()\n",
    "        if self.metric == 'rmse':\n",
    "            return (((y_train - y_pred) ** 2).mean()) ** 0.5\n",
    "        if self.metric == 'mape':\n",
    "            return (np.abs((y_train - y_pred) / y_train) * 100).mean()\n",
    "        if self.metric == 'r2':\n",
    "            return 1 - (((y_train - y_pred) ** 2).sum() / ((y_train - y_train.mean()) ** 2).sum())  \n",
    "\n",
    "    def __regularization(self):\n",
    "        penalty = 0\n",
    "        gradient_penalty = 0\n",
    "        \n",
    "        if self.reg == 'l1':\n",
    "            penalty = self.l1_coef * np.sum(np.abs(self.weights))\n",
    "            gradient_penalty = self.l1_coef * np.sign(self.weights)\n",
    "        elif self.reg == 'l2':\n",
    "            penalty = self.l2_coef * np.sum((self.weights) ** 2)\n",
    "            gradient_penalty = 2 * self.l2_coef * self.weights\n",
    "        elif self.reg == 'elasticnet':\n",
    "            penalty = self.l1_coef * np.sum(np.abs(self.weights)) + self.l2_coef * np.sum((self.weights) ** 2)\n",
    "            gradient_penalty = self.l1_coef * np.sign(self.weights) + 2 * self.l2_coef * self.weights\n",
    "        return penalty, gradient_penalty\n",
    "            \n",
    "    def fit(self, X_train, y_train, verbose=False):\n",
    "        random.seed(self.random_state)\n",
    "        \n",
    "        number_of_observations, number_of_features = X_train.shape\n",
    "        \n",
    "        X_train = np.hstack([np.ones(number_of_observations).reshape(-1, 1), X_train.values])\n",
    "        self.weights = np.ones(number_of_features + 1)\n",
    "        \n",
    "        y_pred = X_train @ self.weights\n",
    "\n",
    "        MSE = ((y_train - y_pred) ** 2).mean()\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"start | loss: {MSE} | {self.metric}: {__metric(y_train, y_pred)}\")\n",
    "\n",
    "        for i in range(1, self.n_iter+1):\n",
    "            penalty, gradient_penalty = self.__regularization()\n",
    "                \n",
    "            y_pred = X_train @ self.weights\n",
    "            MSE = ((y_train - y_pred) ** 2).mean() + penalty\n",
    "\n",
    "            if sgd_sample:\n",
    "                if isinstance(self.sgd_sample, int):\n",
    "                    sgd_idx = random.sample(range(X.shape[0]), self.sgd_sample)\n",
    "                elif isinstance(self.sgd_sample, float):\n",
    "                    part = round(self.sgd_sample * X.shape[0])\n",
    "                    sgd_idx = random.sample(range(X.shape[0]), part)       \n",
    "                X_train_sgd = X_train[sgd_idx]\n",
    "                y_train_sgd = y_train.iloc[sgd_idx]\n",
    "                y_pred_sgd = y_pred[sgd_idx]\n",
    "                gradient = 2 / len(sgd_idx) * X_train_sgd.T @ (y_pred_sgd - y_train_sgd) + gradient_penalty\n",
    "            else:\n",
    "                gradient = 2 / number_of_observations * X_train.T @ (y_pred - y_train) + gradient_penalty\n",
    "            \n",
    "            if callable(self.learning_rate):\n",
    "                learning_rate_temp = self.learning_rate(i)\n",
    "                self.weights -= learning_rate_temp * gradient\n",
    "            else:\n",
    "                self.weights -= self.learning_rate * gradient\n",
    "            \n",
    "            if verbose and i % verbose == 0:\n",
    "                print(f\" {i} | loss: {MSE} | {self.metric}: {__metric(y_train, y_pred)}\")\n",
    "        y_pred = X_train @ self.weights        \n",
    "        self.best_metric = self.__metric(y_train, y_pred)\n",
    "        \n",
    "    def get_coef(self):\n",
    "        return self.weights[1:]\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        number_of_observations, number_of_features = X_test.shape\n",
    "        X_test = np.hstack([np.ones(number_of_observations).reshape(-1, 1), X_test.values])\n",
    "        return X_test @ self.weights\n",
    "\n",
    "    def get_best_score(self):\n",
    "        return self.best_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0669afb0-a804-4fd1-8c20-2c21917fb36d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
