{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b4a3b1c-73a8-4667-a4ea-557af35ce2b2",
   "metadata": {},
   "source": [
    "## **Linear Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc2ec3d5-f0e2-4457-b0a8-02d2a3acb220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class MyLineReg:\n",
    "    def __init__(self, n_iter, learning_rate, weights=None, metric=None, reg=None, l1_coef=0, l2_coef=0, sgd_sample=None, random_state=42):\n",
    "        self.n_iter = n_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = weights\n",
    "        self.metric = metric\n",
    "        self.best_metric = None\n",
    "        self.l1_coef = l1_coef\n",
    "        self.l2_coef = l2_coef\n",
    "        self.reg = reg\n",
    "        self.sgd_sample = sgd_sample\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"MyLineReg class: \\n{' '.join([f'{key} = {value}' for key, value in self.__dict__.items()])}\"\n",
    "\n",
    "    def __mse_loss(self, y, y_pred):\n",
    "        return ((y - y_pred) ** 2).mean()\n",
    "\n",
    "    def __mse_gradient_helper(self, X, y, y_pred):\n",
    "        return X.T @ (y_pred - y)\n",
    "    \n",
    "    def __metric(self, y_train, y_pred):\n",
    "        if self.metric == 'mae':\n",
    "            return (np.abs(y_train - y_pred)).mean()\n",
    "        if self.metric == 'mse':\n",
    "            return ((y_train - y_pred) ** 2).mean()\n",
    "        if self.metric == 'rmse':\n",
    "            return (((y_train - y_pred) ** 2).mean()) ** 0.5\n",
    "        if self.metric == 'mape':\n",
    "            return (np.abs((y_train - y_pred) / y_train) * 100).mean()\n",
    "        if self.metric == 'r2':\n",
    "            return 1 - (((y_train - y_pred) ** 2).sum() / ((y_train - y_train.mean()) ** 2).sum())  \n",
    "\n",
    "    def __regularization(self):\n",
    "        penalty = 0\n",
    "        gradient_penalty = 0\n",
    "        \n",
    "        if self.reg == 'l1':\n",
    "            penalty = self.l1_coef * np.sum(np.abs(self.weights))\n",
    "            gradient_penalty = self.l1_coef * np.sign(self.weights)\n",
    "        elif self.reg == 'l2':\n",
    "            penalty = self.l2_coef * np.sum((self.weights) ** 2)\n",
    "            gradient_penalty = 2 * self.l2_coef * self.weights\n",
    "        elif self.reg == 'elasticnet':\n",
    "            penalty = self.l1_coef * np.sum(np.abs(self.weights)) + self.l2_coef * np.sum((self.weights) ** 2)\n",
    "            gradient_penalty = self.l1_coef * np.sign(self.weights) + 2 * self.l2_coef * self.weights\n",
    "        return penalty, gradient_penalty\n",
    "\n",
    "    def __sgd_helper(self, X_train, y_train, y_pred):\n",
    "        if isinstance(self.sgd_sample, int):\n",
    "            sgd_idx = random.sample(range(X.shape[0]), self.sgd_sample)\n",
    "        elif isinstance(self.sgd_sample, float):\n",
    "            part = round(self.sgd_sample * X.shape[0])\n",
    "            sgd_idx = random.sample(range(X.shape[0]), part)\n",
    "        X_train_sgd = X_train[sgd_idx]\n",
    "        y_train_sgd = y_train.iloc[sgd_idx]\n",
    "        y_pred_sgd = y_pred[sgd_idx]\n",
    "        gradient = 2 / len(sgd_idx) * self.__mse_gradient_helper(X_train_sgd, y_train_sgd, y_pred_sgd)\n",
    "        return gradient\n",
    "            \n",
    "    def fit(self, X_train, y_train, verbose=False):\n",
    "        random.seed(self.random_state)\n",
    "        \n",
    "        number_of_observations, number_of_features = X_train.shape\n",
    "        \n",
    "        X_train = np.hstack([np.ones(number_of_observations).reshape(-1, 1), X_train.values])\n",
    "        \n",
    "        if not self.weights:\n",
    "            self.weights = np.ones(number_of_features+1)\n",
    "        \n",
    "        y_pred = X_train @ self.weights\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"start | loss: {MSE} | {self.metric}: {self.__metric(y_train, y_pred)}\")\n",
    "\n",
    "        for i in range(1, self.n_iter+1):\n",
    "            penalty, gradient_penalty = self.__regularization()\n",
    "                \n",
    "            y_pred = X_train @ self.weights\n",
    "            loss = self.__mse_loss(y_train, y_pred) + penalty\n",
    "\n",
    "            if sgd_sample:\n",
    "                gradient = self.__sgd_helper(X_train, y_train, y_pred) + gradient_penalty\n",
    "            else:\n",
    "                gradient = 2 / number_of_observations * self.__mse_gradient_helper(X_train, y_train, y_pred) + gradient_penalty\n",
    "            \n",
    "            if callable(self.learning_rate):\n",
    "                learning_rate_temp = self.learning_rate(i)\n",
    "                self.weights -= learning_rate_temp * gradient\n",
    "            else:\n",
    "                self.weights -= self.learning_rate * gradient\n",
    "            \n",
    "            if verbose and i % verbose == 0:\n",
    "                print(f\" {i} | loss: {MSE} | {self.metric}: {self.__metric(y_train, y_pred)}\")\n",
    "        y_pred = X_train @ self.weights        \n",
    "        self.best_metric = self.__metric(y_train, y_pred)\n",
    "        \n",
    "    def get_coef(self):\n",
    "        return self.weights[1:]\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        number_of_observations, number_of_features = X_test.shape\n",
    "        X_test = np.hstack([np.ones(number_of_observations).reshape(-1, 1), X_test.values])\n",
    "        return X_test @ self.weights\n",
    "\n",
    "    def get_best_score(self):\n",
    "        return self.best_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b25507-538a-4b01-8088-63f7e05c64e6",
   "metadata": {},
   "source": [
    "## **Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea95035e-33d6-40bf-bfbc-6a7c987ba0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class MyLogReg:\n",
    "    def __init__(self, n_iter, learning_rate, weights=None, metric=None, reg=None, l1_coef=0, l2_coef=0, sgd_sample=None, random_state=42):\n",
    "        self.n_iter = n_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = weights\n",
    "        self.metric = metric\n",
    "        self.__best_metric = None\n",
    "        self.reg = reg\n",
    "        self.l1_coef = l1_coef\n",
    "        self.l2_coef = l2_coef\n",
    "        self.sgd_sample = sgd_sample\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"MyLineReg class: \\n{' '.join([f'{key} = {value}' for key, value in self.__dict__.items()])}\"\n",
    "\n",
    "    def __sigmoid(self, value):\n",
    "        return 1 / (1 + np.exp(-value))\n",
    "\n",
    "    def __log_loss(self, y, y_pred):\n",
    "        eps = 1e-15\n",
    "        return -(y * np.log(y_pred + eps) + (1 - y) * np.log(1 - y_pred + eps)).mean()\n",
    "\n",
    "    def __log_gradient_helper(self, X, y, y_pred):\n",
    "        return ((y_pred - y) @ X)\n",
    "\n",
    "    def __predict_helper(self, X):\n",
    "        return self.__sigmoid(X @ self.weights)\n",
    "\n",
    "    def __metric(self, y_train, y_scores):\n",
    "        y_pred = np.where(y_scores > 0.5, 1, 0)\n",
    "        \n",
    "        TP = np.sum((y_train==1) & (y_pred==1))\n",
    "        FP = np.sum((y_train==0) & (y_pred==1))\n",
    "        FN = np.sum((y_train==1) & (y_pred==0))\n",
    "        TN = np.sum((y_train==0) & (y_pred==0))\n",
    "        \n",
    "        if self.metric == 'accuracy':\n",
    "            return (TP + TN) / (TP + TN + FP + FN)\n",
    "        if self.metric == 'precision':\n",
    "            return TP / (TP + FP)\n",
    "        if self.metric == 'recall':\n",
    "            return TP / (TP + FN)\n",
    "        if self.metric == 'f1':\n",
    "            precision = TP / (TP + FP)\n",
    "            recall = TP / (TP + FN) \n",
    "            return 2 * precision * recall / (precision + recall) \n",
    "        if self.metric == 'roc_auc':\n",
    "            data = pd.DataFrame({'y_true': y_train, 'y_scores': y_scores})\n",
    "            data = data.sort_values(by='y_scores', ascending=False)\n",
    "        \n",
    "            P = np.sum(data['y_true'] == 1)\n",
    "            N = np.sum(data['y_true'] == 0)\n",
    "        \n",
    "            cumulative_positives = 0\n",
    "            auc = 0\n",
    "        \n",
    "            for _, row in data.iterrows():\n",
    "                if row['y_true'] == 1:\n",
    "                    cumulative_positives += 1\n",
    "                else:\n",
    "                    auc += cumulative_positives\n",
    "        \n",
    "            return auc / (P * N)\n",
    "\n",
    "    def __regularization(self):\n",
    "        penalty = 0\n",
    "        gradient_penalty = 0\n",
    "        \n",
    "        if self.reg == 'l1':\n",
    "            penalty = self.l1_coef * np.sum(np.abs(self.weights))\n",
    "            gradient_penalty = self.l1_coef * np.sign(self.weights)\n",
    "        elif self.reg == 'l2':\n",
    "            penalty = self.l2_coef * np.sum((self.weights) ** 2)\n",
    "            gradient_penalty = 2 * self.l2_coef * self.weights\n",
    "        elif self.reg == 'elasticnet':\n",
    "            penalty = self.l1_coef * np.sum(np.abs(self.weights)) + self.l2_coef * np.sum((self.weights) ** 2)\n",
    "            gradient_penalty = self.l1_coef * np.sign(self.weights) + 2 * self.l2_coef * self.weights\n",
    "        return penalty, gradient_penalty\n",
    "\n",
    "    def __sgd_helper(self, X_train, y_train, y_pred):\n",
    "        if isinstance(self.sgd_sample, int):\n",
    "            sgd_idx = random.sample(range(X.shape[0]), self.sgd_sample)\n",
    "        elif isinstance(self.sgd_sample, float):\n",
    "            part = round(self.sgd_sample * X.shape[0])\n",
    "            sgd_idx = random.sample(range(X.shape[0]), part)\n",
    "        X_train_sgd = X_train[sgd_idx]\n",
    "        y_train_sgd = y_train.iloc[sgd_idx]\n",
    "        y_pred_sgd = y_pred[sgd_idx]\n",
    "        gradient = self.__log_gradient_helper(X_train_sgd, y_train_sgd, y_pred_sgd) / len(sgd_idx)\n",
    "        return gradient\n",
    "    \n",
    "    def fit(self, X_train, y_train, verbose=False):\n",
    "        random.seed(self.random_state)\n",
    "\n",
    "        number_of_observations, number_of_features = X_train.shape\n",
    "        \n",
    "        X_train = np.hstack([np.ones(number_of_observations).reshape(-1, 1), X_train.values])\n",
    "\n",
    "        if not self.weights:\n",
    "            self.weights = np.ones(number_of_features + 1)\n",
    "\n",
    "        y_pred = self.__predict_helper(X_train)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"start | loss: {loss} | {self.metric}: {self.__metric(y_train, y_pred)}\")\n",
    "\n",
    "        for i in range(1, self.n_iter+1):\n",
    "            penalty, gradient_penalty = self.__regularization()\n",
    "            \n",
    "            y_pred = self.__predict_helper(X_train)\n",
    "            loss = self.__log_loss(y_train, y_pred) + penalty\n",
    "            \n",
    "            if sgd_sample:\n",
    "                gradient = self.__sgd_helper(X_train, y_train, y_pred) + gradient_penalty\n",
    "            else:\n",
    "                gradient = self.__log_gradient_helper(X_train, y_train, y_pred) / number_of_observations + gradient_penalty\n",
    "\n",
    "            if callable(self.learning_rate):\n",
    "                learning_rate_temp = self.learning_rate(i)\n",
    "                self.weights -= learning_rate_temp * gradient\n",
    "            else:\n",
    "                self.weights -= self.learning_rate * gradient\n",
    "\n",
    "            if verbose and i % verbose == 0:\n",
    "                    print(f\" {i} | loss: {loss} | {self.metric}: {self.__metric(y_train, y_pred)}\")\n",
    "                \n",
    "        y_pred = self.__predict_helper(X_train)\n",
    "        self.__best_metric = self.__metric(y_train, y_pred)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        number_of_observations, number_of_features = X_test.shape\n",
    "\n",
    "        if type(X_test) == pd.core.frame.DataFrame: \n",
    "            X_test = np.hstack([np.ones(number_of_observations).reshape(-1, 1), X_test.values])\n",
    "        probability = self.__predict_helper(X_test)\n",
    "        y_pred = np.where(probability > 0.5, 1, 0)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "    def predict_proba(self, X_test):\n",
    "        number_of_observations, number_of_features = X_test.shape\n",
    "        \n",
    "        X_test = np.hstack([np.ones(number_of_observations).reshape(-1, 1), X_test.values])\n",
    "\n",
    "        probability = self.__predict_helper(X_test)\n",
    "        \n",
    "        return probability\n",
    "\n",
    "    def get_coef(self):\n",
    "        return self.weights[1:]\n",
    "\n",
    "    def get_best_score(self):\n",
    "        return self.__best_metric"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
